{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from lazypredict.Supervised import LazyRegressor\n",
        "\n",
        "# Cargar datos\n",
        "train = pd.read_csv(\"dengue_features_train.csv\")\n",
        "train_labels = pd.read_csv(\"dengue_labels_train.csv\")\n",
        "test = pd.read_csv(\"dengue_features_test.csv\")\n",
        "\n",
        "# Unir etiquetas al conjunto de entrenamiento\n",
        "train = train.merge(train_labels, on=[\"city\", \"year\", \"weekofyear\"])\n",
        "\n",
        "# Convertir fecha a datetime y eliminar la columna\n",
        "train[\"week_start_date\"] = pd.to_datetime(train[\"week_start_date\"])\n",
        "test[\"week_start_date\"] = pd.to_datetime(test[\"week_start_date\"])\n",
        "train.drop(columns=[\"week_start_date\"], inplace=True)\n",
        "test.drop(columns=[\"week_start_date\"], inplace=True)\n",
        "\n",
        "# Seleccionar solo las columnas numéricas para la imputación (excluyendo city, year, weekofyear, total_cases)\n",
        "num_features = train.columns.difference([\"city\", \"year\", \"weekofyear\", \"total_cases\"])\n",
        "\n",
        "# Imputar valores faltantes con la media solo en las columnas numéricas\n",
        "imputer = SimpleImputer(strategy=\"mean\")\n",
        "train[num_features] = imputer.fit_transform(train[num_features])\n",
        "test[num_features] = imputer.transform(test[num_features])\n",
        "\n",
        "# Separar características y etiquetas\n",
        "X = train.drop(columns=[\"total_cases\"])\n",
        "y = train[\"total_cases\"]\n",
        "\n",
        "# Normalizar datos solo en las columnas numéricas\n",
        "scaler = StandardScaler()\n",
        "X[num_features] = scaler.fit_transform(X[num_features])\n",
        "test[num_features] = scaler.transform(test[num_features])\n",
        "\n",
        "# Dividir datos en entrenamiento y prueba\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Lazy Predict para evaluar modelos automáticamente\n",
        "lazy_reg = LazyRegressor()\n",
        "models, predictions = lazy_reg.fit(X_train[num_features], X_val[num_features], y_train, y_val)\n",
        "print(models)\n",
        "\n",
        "# Naive Bayes\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train[num_features], y_train)\n",
        "y_pred_nb = nb.predict(X_val[num_features])\n",
        "mae_nb = mean_absolute_error(y_val, y_pred_nb)\n",
        "print(f\"MAE Naive Bayes: {mae_nb}\")\n",
        "\n",
        "# KNN\n",
        "knn = KNeighborsRegressor(n_neighbors=5)\n",
        "knn.fit(X_train[num_features], y_train)\n",
        "y_pred_knn = knn.predict(X_val[num_features])\n",
        "mae_knn = mean_absolute_error(y_val, y_pred_knn)\n",
        "print(f\"MAE KNN: {mae_knn}\")\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train[num_features], y_train)\n",
        "y_pred_rf = rf.predict(X_val[num_features])\n",
        "mae_rf = mean_absolute_error(y_val, y_pred_rf)\n",
        "print(f\"MAE Random Forest: {mae_rf}\")\n",
        "\n",
        "# GridSearch en KNN\n",
        "param_grid = {\"n_neighbors\": [3, 5, 7, 9, 11]}\n",
        "grid_search_knn = GridSearchCV(KNeighborsRegressor(), param_grid, scoring=\"neg_mean_absolute_error\", cv=3)\n",
        "grid_search_knn.fit(X_train[num_features], y_train)\n",
        "print(\"Mejor K para KNN:\", grid_search_knn.best_params_)\n",
        "\n",
        "# RandomizedSearch en Random Forest\n",
        "param_dist = {\n",
        "    \"n_estimators\": [50, 100, 200, 300],\n",
        "    \"max_depth\": [None, 10, 20, 30],\n",
        "    \"min_samples_split\": [2, 5, 10],\n",
        "    \"min_samples_leaf\": [1, 2, 4]\n",
        "}\n",
        "random_search_rf = RandomizedSearchCV(RandomForestRegressor(), param_distributions=param_dist, n_iter=10, scoring=\"neg_mean_absolute_error\", cv=3, random_state=42)\n",
        "random_search_rf.fit(X_train[num_features], y_train)\n",
        "print(\"Mejores hiperparámetros para Random Forest:\", random_search_rf.best_params_)\n",
        "\n",
        "# Generar predicciones finales\n",
        "best_rf = random_search_rf.best_estimator_\n",
        "final_predictions = best_rf.predict(test[num_features])\n",
        "\n",
        "# Crear DataFrame de envío\n",
        "submission = test[[\"city\", \"year\", \"weekofyear\"]].copy()\n",
        "submission[\"total_cases\"] = np.round(final_predictions).astype(int)\n",
        "\n",
        "# Guardar CSV\n",
        "submission.to_csv(\"submission.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na9bYSRLzey2",
        "outputId": "247eefa4-bc62-40ea-e679-0c66c9cc11ce"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 42/42 [00:13<00:00,  3.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000273 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4412\n",
            "[LightGBM] [Info] Number of data points in the train set: 1164, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 23.116838\n",
            "                               Adjusted R-Squared  R-Squared  RMSE  Time Taken\n",
            "Model                                                                         \n",
            "GradientBoostingRegressor                    0.39       0.43 42.63        1.43\n",
            "RandomForestRegressor                        0.37       0.42 43.24        2.24\n",
            "AdaBoostRegressor                            0.34       0.39 44.43        0.77\n",
            "XGBRegressor                                 0.34       0.38 44.57        0.56\n",
            "HistGradientBoostingRegressor                0.33       0.38 44.71        0.61\n",
            "BaggingRegressor                             0.33       0.37 44.87        0.52\n",
            "LGBMRegressor                                0.32       0.37 45.03        0.21\n",
            "ExtraTreesRegressor                          0.30       0.35 45.60        2.43\n",
            "KNeighborsRegressor                          0.23       0.28 48.09        0.08\n",
            "DecisionTreeRegressor                        0.20       0.26 48.84        0.14\n",
            "MLPRegressor                                 0.07       0.14 52.63        1.28\n",
            "GaussianProcessRegressor                     0.06       0.13 52.93        0.32\n",
            "PoissonRegressor                             0.06       0.12 53.09        0.03\n",
            "TransformedTargetRegressor                   0.05       0.11 53.35        0.02\n",
            "LinearRegression                             0.05       0.11 53.35        0.04\n",
            "Ridge                                        0.05       0.11 53.35        0.02\n",
            "RidgeCV                                      0.05       0.11 53.38        0.02\n",
            "Lars                                         0.05       0.11 53.39        0.09\n",
            "SGDRegressor                                 0.05       0.11 53.41        0.02\n",
            "LassoLarsIC                                  0.04       0.11 53.48        0.13\n",
            "ElasticNetCV                                 0.04       0.11 53.58        0.45\n",
            "BayesianRidge                                0.04       0.11 53.60        0.10\n",
            "LassoLarsCV                                  0.04       0.10 53.67        0.08\n",
            "LassoCV                                      0.04       0.10 53.68        0.29\n",
            "LarsCV                                       0.03       0.10 53.80        0.10\n",
            "LassoLars                                    0.03       0.10 53.85        0.02\n",
            "Lasso                                        0.03       0.10 53.85        0.05\n",
            "OrthogonalMatchingPursuit                    0.03       0.09 53.93        0.02\n",
            "ElasticNet                                   0.02       0.09 54.07        0.04\n",
            "OrthogonalMatchingPursuitCV                  0.02       0.09 54.10        0.03\n",
            "TweedieRegressor                             0.02       0.09 54.13        0.02\n",
            "NuSVR                                       -0.06       0.01 56.42        0.13\n",
            "HuberRegressor                              -0.07       0.01 56.44        0.22\n",
            "PassiveAggressiveRegressor                  -0.08      -0.00 56.69        0.02\n",
            "SVR                                         -0.08      -0.01 56.87        0.15\n",
            "LinearSVR                                   -0.09      -0.01 57.01        0.05\n",
            "DummyRegressor                              -0.09      -0.02 57.19        0.07\n",
            "QuantileRegressor                           -0.19      -0.11 59.72        0.13\n",
            "KernelRidge                                 -0.24      -0.15 60.84        0.40\n",
            "ExtraTreeRegressor                          -0.29      -0.20 62.05        0.09\n",
            "RANSACRegressor                             -0.43      -0.34 65.47        0.13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE Naive Bayes: 29.86986301369863\n",
            "MAE KNN: 23.84931506849315\n",
            "MAE Random Forest: 22.15398503750815\n",
            "Mejor K para KNN: {'n_neighbors': 11}\n",
            "Mejores hiperparámetros para Random Forest: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_depth': 20}\n"
          ]
        }
      ]
    }
  ]
}